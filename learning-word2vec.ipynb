{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found and verified text8.zip\n",
      "Data size 17005207\n",
      "initialized\n",
      "Average loss at step 0 : 265.249664307\n",
      "Nearst to states: severity, hm, defer, sykes, pls, embassies, postwar, frasier,\n",
      "Average loss at step 2000 : 114.486019419\n",
      "Average loss at step 4000 : 52.2571521373\n",
      "Average loss at step 6000 : 33.4842949784\n",
      "Average loss at step 8000 : 23.3282863998\n",
      "Average loss at step 10000 : 17.4727575295\n",
      "Nearst to states: severity, hm, pls, size, rotation, atoms, inflation, spacecraft,\n",
      "Average loss at step 12000 : 14.405830705\n",
      "Average loss at step 14000 : 11.7090381358\n",
      "Average loss at step 16000 : 9.68055750513\n",
      "Average loss at step 18000 : 8.57548885834\n",
      "Average loss at step 20000 : 7.71688210106\n",
      "Nearst to states: severity, hm, pls, size, rotation, embassies, inflation, atoms,\n",
      "Average loss at step 22000 : 7.22766306627\n",
      "Average loss at step 24000 : 7.01210313058\n",
      "Average loss at step 26000 : 6.73778890646\n",
      "Average loss at step 28000 : 6.27526145101\n",
      "Average loss at step 30000 : 6.15797633338\n",
      "Nearst to states: severity, hm, appomattox, pls, size, rotation, embassies, territory,\n",
      "Average loss at step 32000 : 5.90161170363\n",
      "Average loss at step 34000 : 5.8732086904\n",
      "Average loss at step 36000 : 5.67045183754\n",
      "Average loss at step 38000 : 5.26417990065\n",
      "Average loss at step 40000 : 5.44898710001\n",
      "Nearst to states: severity, appomattox, hm, territory, size, pls, mcduck, postwar,\n",
      "Average loss at step 42000 : 5.315774418\n",
      "Average loss at step 44000 : 5.29676056302\n",
      "Average loss at step 46000 : 5.24673794448\n",
      "Average loss at step 48000 : 5.04167924619\n",
      "Average loss at step 50000 : 5.17913367212\n",
      "Nearst to states: severity, hm, appomattox, territory, postwar, recruitment, size, pls,\n",
      "Average loss at step 52000 : 5.15847725868\n",
      "Average loss at step 54000 : 5.14172615576\n",
      "Average loss at step 56000 : 5.06363355768\n",
      "Average loss at step 58000 : 5.07802770221\n",
      "Average loss at step 60000 : 4.93566683573\n",
      "Nearst to states: severity, hm, appomattox, territory, recruitment, michelob, postwar, succumbing,\n",
      "Average loss at step 62000 : 4.79328186584\n",
      "Average loss at step 64000 : 4.78268830669\n",
      "Average loss at step 66000 : 4.97069887507\n",
      "Average loss at step 68000 : 4.90549153161\n",
      "Average loss at step 70000 : 4.77506577718\n",
      "Nearst to states: severity, appomattox, hm, michelob, territory, recruitment, frasier, postwar,\n",
      "Average loss at step 72000 : 4.81290925622\n",
      "Average loss at step 74000 : 4.78029521233\n",
      "Average loss at step 76000 : 4.86335469341\n",
      "Average loss at step 78000 : 4.80131394207\n",
      "Average loss at step 80000 : 4.80604526174\n",
      "Nearst to states: severity, appomattox, hm, territory, michelob, recruitment, postwar, frasier,\n",
      "Average loss at step 82000 : 4.80682219088\n",
      "Average loss at step 84000 : 4.77658238578\n",
      "Average loss at step 86000 : 4.73572464573\n",
      "Average loss at step 88000 : 4.68823356938\n",
      "Average loss at step 90000 : 4.75057739556\n",
      "Nearst to states: severity, hm, appomattox, abies, territory, michelob, recruitment, seaton,\n",
      "Average loss at step 92000 : 4.70391439235\n",
      "Average loss at step 94000 : 4.62350165462\n",
      "Average loss at step 96000 : 4.72568217933\n",
      "Average loss at step 98000 : 4.63293865234\n",
      "Average loss at step 100000 : 4.67366400456\n",
      "Nearst to states: severity, hm, abies, appomattox, territory, seaton, recruitment, succumbing,\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('failed to verified ' + filename + '. can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    # print(data)\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))\n",
    "\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]    #存储单词及对应出现的次数\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()      #存储单词及对应的编号\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)  #词频高的单词编号小\n",
    "    data = list()    #将文章存储为编号的形式\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "\n",
    "del words\n",
    "\n",
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        target_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in target_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            target_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "# batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "# for i in range(8):\n",
    "#     print('batch', batch[i], reverse_dictionary[batch[i]])\n",
    "#     print('couple', batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skip = 2\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sample = 64 #噪声词数量\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs) #函数作用上文解释\n",
    "\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=train_labels,\n",
    "                                         inputs=embed,\n",
    "                                         num_sampled=num_sample,\n",
    "                                         num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    #验证相似度\n",
    "    norm = tf.sqrt(tf.reduce_mean(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print('initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skip, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print('Average loss at step', step, ':', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearst = (-sim[i, :]).argsort()[1: top_k+1]\n",
    "                log_str = \"Nearst to %s:\" % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearst[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000) #TSNE降维函数\n",
    "plot_only = 100\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
