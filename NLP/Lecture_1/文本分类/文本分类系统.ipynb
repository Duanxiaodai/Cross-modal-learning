{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-c32523fde0b4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c32523fde0b4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    文本挖掘是指从大量文本数据中抽取实现未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参考。即从非结构化的文本中寻找知识的过程。\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "一个小型的文本分类系统-python（文末附语料，停用词文本文档，工程全部代码）\n",
    "\n",
    "https://www.cnblogs.com/kevinzhaozl/p/6625110.html\n",
    "\n",
    "背景\n",
    "文本挖掘是指从大量文本数据中抽取实现未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参考。即从非结构化的文本中寻找知识的过程。\n",
    "\n",
    "目前文本挖掘主要有7个主要领域：\n",
    "\n",
    "· 搜索和信息检索IR\n",
    "· 文本聚类：使用聚类方法对词汇、片段、段落或文件进行分组和归类\n",
    "· 文本分类：对片段、段落或文件进行分组和归类，在使用数据挖掘分类方法的基础上，经过训练地标记实例模型\n",
    "· Web挖掘：在互联网上进行数据和文本挖掘，病特别关注网络的规模和相互联系\n",
    "· 信息抽取IE：从非结构文本中识别和抽取有关的事实和关系；从非结构化或者半结构化的文本中抽取结构化的抽取出结构化数据的过程\n",
    "· 自然语言处理NLP：从语法、语义的角度发现语言本质的结构和所表达的意义\n",
    "文本分类系统（python 3.5）\n",
    "中文语言的文本分类技术和流程，主要包括下面几个步骤：\n",
    "1. 预处理：去除文本噪声信息，例如HTML标签，文本格式转换，检测句子边界\n",
    "\n",
    "2. 中文分词：使用中文分词器为文本分词，并去除停用词\n",
    "\n",
    "3. 构建词向量空间：统计文本词频，生成文本的词向量空间\n",
    "\n",
    "4. 权重策略——TF-IDF：使用TF-IDF发现特征词，并抽取为反映文档主题的特征\n",
    "\n",
    "5. 分类词：使用算法训练分类器\n",
    "\n",
    "6. 评价分类结果\n",
    "\n",
    " \n",
    "\n",
    "1. 预处理\n",
    "\n",
    "a. 选择处理文本的范围\n",
    "\n",
    "b. 建立分类文本语料库\n",
    "\n",
    "· 训练集语料\n",
    "已经分好类的文本资源\n",
    "\n",
    "· 测试集语料\n",
    "待分类的文本语料，可以使训练集的一部分，也可以是外部来源的文本语料\n",
    "\n",
    "c. 文本格式转化：使用Python的lxml库去除html标签\n",
    "\n",
    "d. 检测句子边界：标记句子的结束\n",
    "\n",
    "2. 中文分词\n",
    "\n",
    "分词是将连续的字序列按照一定的规范重新组合成词序列的过程，中文分词即将一个汉字序列（句子）切分成一个个独立的单词，中文分词很复杂，从某种程度上并不完全是一个算法问题，最终概率论解决了这个问题，算法是基于概率图模型的条件随机场（CRF）\n",
    "\n",
    "分词是自然语言处理中最基本，最底层的模块，分词精度对后续应用模块的影响很大，文本或句子的结构化表示是语言处理中最核心的任务，目前文本的结构化表示分为四大类：词向量空间、主体模型、依存句法的树表示、RDF的图表示。\n",
    "\n",
    "下面给出中文词的示例代码：\n",
    "\n",
    " \n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "\n",
    "def savefile(savepath, content):\n",
    "    fp = open(savepath, \"w\",encoding='gb2312', errors='ignore')\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "def readfile(path):\n",
    "    fp = open(path, \"r\", encoding='gb2312', errors='ignore')\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "# corpus_path = \"train_small/\"  # 未分词分类预料库路径\n",
    "# seg_path = \"train_seg/\"  # 分词后分类语料库路径\n",
    "\n",
    "corpus_path = \"test_small/\"  # 未分词分类预料库路径\n",
    "seg_path = \"test_seg/\"  # 分词后分类语料库路径\n",
    "\n",
    "catelist = os.listdir(corpus_path)  # 获取改目录下所有子目录\n",
    "\n",
    "for mydir in catelist:\n",
    "    class_path = corpus_path + mydir + \"/\"  # 拼出分类子目录的路径\n",
    "    seg_dir = seg_path + mydir + \"/\"  # 拼出分词后预料分类目录\n",
    "    if not os.path.exists(seg_dir):  # 是否存在，不存在则创建\n",
    "        os.makedirs(seg_dir)\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path + file_path\n",
    "        content = readfile(fullname).strip()  # 读取文件内容\n",
    "        content = content.replace(\"\\r\\n\", \"\").strip()  # 删除换行和多余的空格\n",
    "        content_seg = jieba.cut(content)\n",
    "        savefile(seg_dir + file_path, \" \".join(content_seg))\n",
    "\n",
    "print(\"分词结束\")\n",
    "\n",
    " \n",
    "\n",
    "为了后续生成词向量空间模型的方便，这些分词后的文本信息还要转换成文本向量信息并对象化，利用了Scikit-Learn库的Bunch数据结构，具体代码如下：\n",
    "\n",
    " \n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "#Bunch 类提供了一种key，value的对象形式\n",
    "\n",
    "#target_name 所有分类集的名称列表\n",
    "\n",
    "#label 每个文件的分类标签列表\n",
    "\n",
    "#filenames 文件路径\n",
    "\n",
    "#contents 分词后文件词向量形式\n",
    "\n",
    "def readfile(path):\n",
    "\n",
    "    fp = open(path, \"r\", encoding='gb2312', errors='ignore')\n",
    "\n",
    "    content = fp.read()\n",
    "\n",
    "    fp.close()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "bunch=Bunch(target_name=[],label=[],filenames=[],contents=[])\n",
    "\n",
    "\n",
    "\n",
    "# wordbag_path=\"train_word_bag/train_set.dat\"\n",
    "\n",
    "# seg_path=\"train_seg/\"\n",
    "\n",
    "wordbag_path=\"test_word_bag/test_set.dat\"\n",
    "\n",
    "seg_path=\"test_seg/\"\n",
    "\n",
    "\n",
    "\n",
    "catelist=os.listdir(seg_path)\n",
    "\n",
    "bunch.target_name.extend(catelist)#将类别信息保存到Bunch对象\n",
    "\n",
    "for mydir in catelist:\n",
    "\n",
    "    class_path=seg_path+mydir+\"/\"\n",
    "\n",
    "    file_list=os.listdir(class_path)\n",
    "\n",
    "    for file_path in file_list:\n",
    "\n",
    "        fullname=class_path+file_path\n",
    "\n",
    "        bunch.label.append(mydir)#保存当前文件的分类标签\n",
    "\n",
    "        bunch.filenames.append(fullname)#保存当前文件的文件路径\n",
    "\n",
    "        bunch.contents.append(readfile(fullname).strip())#保存文件词向量\n",
    "\n",
    "\n",
    "\n",
    "#Bunch对象持久化\n",
    "\n",
    "file_obj=open(wordbag_path,\"wb\")\n",
    "\n",
    "pickle.dump(bunch,file_obj)\n",
    "\n",
    "file_obj.close()\n",
    "\n",
    "\n",
    "\n",
    "print(\"构建文本对象结束\")\n",
    " \n",
    "\n",
    "3. 向量空间模型\n",
    "\n",
    "由于文本在储存未向量空间是维度较高，为节省储存空间和提高搜索效率，在文本分类之前会自动过滤掉某些字词，这些字或词被称为停用词，停用此表可以到点这里下载。\n",
    "\n",
    "4. 权重策略：TF-IDF方法\n",
    "\n",
    "如果某个词或短语在一篇文章中出现的频率高，并且在其他文章中很少出现，那么认为这个词或者短语具有很好的类别区分能力，适合用来分类。\n",
    "\n",
    "再给出这部分代码之前，我们先来看词频和逆向文件频率的概念\n",
    "\n",
    "词频（TF）：指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数的归一化，以防止它偏向长的文件，对于某一个特定文件里的词语来说，它的重要性可表示为：\n",
    "\n",
    " \n",
    "\n",
    "分子是该词在文件中出现的次数，分母是在文件中所有字词的出现次数之和\n",
    "\n",
    "逆向文件频率（IDF）是一个词语普遍重要性的度量，某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数：\n",
    "\n",
    " \n",
    "\n",
    "|D|是语料库中的文件总数，j是包含词语的文件数目，如果该词语不在语料库中，就会导致分母为零，因此一般情况下分母还要额外再加上1\n",
    "\n",
    "之后计算词频和逆向文件频率的乘积，某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF，因此TF-IDF倾向于过滤掉常见的词语，保留重要的词语。代码如下：\n",
    "\n",
    " \n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "import pickle#持久化类\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer#TF-IDF向量转换类\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#TF-IDF向量生成类\n",
    "\n",
    "\n",
    "\n",
    "def readbunchobj(path):\n",
    "\n",
    "    file_obj=open(path,\"rb\")\n",
    "\n",
    "    bunch=pickle.load(file_obj)\n",
    "\n",
    "    file_obj.close()\n",
    "\n",
    "    return bunch\n",
    "\n",
    "def writebunchobj(path,bunchobj):\n",
    "\n",
    "    file_obj=open(path,\"wb\")\n",
    "\n",
    "    pickle.dump(bunchobj,file_obj)\n",
    "\n",
    "    file_obj.close()\n",
    "\n",
    "\n",
    "\n",
    "def readfile(path):\n",
    "\n",
    "    fp = open(path, \"r\", encoding='gb2312', errors='ignore')\n",
    "\n",
    "    content = fp.read()\n",
    "\n",
    "    fp.close()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"train_word_bag/train_set.dat\"\n",
    "\n",
    "bunch=readbunchobj(path)\n",
    "\n",
    "\n",
    "\n",
    "#停用词\n",
    "\n",
    "stopword_path=\"train_word_bag/hlt_stop_words.txt\"\n",
    "\n",
    "stpwrdlst=readfile(stopword_path).splitlines()\n",
    "\n",
    "#构建TF-IDF词向量空间对象\n",
    "\n",
    "tfidfspace=Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames,tdm=[],vocabulary={})\n",
    "\n",
    "#使用TfidVectorizer初始化向量空间模型\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf=True,max_df=0.5)\n",
    "\n",
    "transfoemer=TfidfTransformer()#该类会统计每个词语的TF-IDF权值\n",
    "\n",
    "\n",
    "\n",
    "#文本转为词频矩阵，单独保存字典文件\n",
    "\n",
    "tfidfspace.tdm=vectorizer.fit_transform(bunch.contents)\n",
    "\n",
    "tfidfspace.vocabulary=vectorizer.vocabulary_\n",
    "\n",
    "\n",
    "\n",
    "#创建词袋的持久化\n",
    "\n",
    "space_path=\"train_word_bag/tfidfspace.dat\"\n",
    "\n",
    "writebunchobj(space_path,tfidfspace)\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "5.使用朴素贝叶斯分类模块\n",
    "\n",
    "常用的文本分类方法有kNN最近邻法，朴素贝叶斯算法和支持向量机算法，一般而言：\n",
    "\n",
    "kNN算法原来最简单，分类精度尚可，但是速度最快支\n",
    "\n",
    "朴素贝叶斯算法对于短文本分类的效果最好，精度很高\n",
    "\n",
    "支持向量机算法的优势是支持线性不可分的情况，精度上取中\n",
    "\n",
    "上文代码中进行操作的都是训练集的数据，下面是测试集（抽取字训练集），训练步骤和训练集相同，首先是分词，之后生成词向量文件，直至生成词向量模型，不同的是，在训练词向量模型时需要加载训练集词袋，将测试集产生的词向量映射到训练集词袋的词典中，生成向量空间模型，代码如下：\n",
    "\n",
    " \n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "import pickle#持久化类\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer#TF-IDF向量转换类\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#TF-IDF向量生成类\n",
    "\n",
    "\n",
    "\n",
    "from TF_IDF import space_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readbunchobj(path):\n",
    "\n",
    "    file_obj=open(path,\"rb\")\n",
    "\n",
    "    bunch=pickle.load(file_obj)\n",
    "\n",
    "    file_obj.close()\n",
    "\n",
    "    return bunch\n",
    "\n",
    "def writebunchobj(path,bunchobj):\n",
    "\n",
    "    file_obj=open(path,\"wb\")\n",
    "\n",
    "    pickle.dump(bunchobj,file_obj)\n",
    "\n",
    "    file_obj.close()\n",
    "\n",
    "def readfile(path):\n",
    "\n",
    "    fp = open(path, \"r\", encoding='gb2312', errors='ignore')\n",
    "\n",
    "    content = fp.read()\n",
    "\n",
    "    fp.close()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "#导入分词后的词向量bunch对象\n",
    "\n",
    "path=\"test_word_bag/test_set.dat\"\n",
    "\n",
    "bunch=readbunchobj(path)\n",
    "\n",
    "\n",
    "\n",
    "#停用词\n",
    "\n",
    "stopword_path=\"train_word_bag/hlt_stop_words.txt\"\n",
    "\n",
    "stpwrdlst=readfile(stopword_path).splitlines()\n",
    "\n",
    "\n",
    "\n",
    "#构建测试集TF-IDF向量空间\n",
    "\n",
    "testspace=Bunch(target_name=bunch.target_name,label=bunch.label,filenames=bunch.filenames,tdm=[],vocabulary={})\n",
    "\n",
    "\n",
    "\n",
    "#导入训练集的词袋\n",
    "\n",
    "trainbunch=readbunchobj(\"train_word_bag/tfidfspace.dat\")\n",
    "\n",
    "\n",
    "\n",
    "#使用TfidfVectorizer初始化向量空间\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stpwrdlst,sublinear_tf=True,max_df=0.5,vocabulary=trainbunch.vocabulary)\n",
    "\n",
    "transformer=TfidfTransformer();\n",
    "\n",
    "testspace.tdm=vectorizer.fit_transform(bunch.contents)\n",
    "\n",
    "testspace.vocabulary=trainbunch.vocabulary\n",
    "\n",
    "\n",
    "\n",
    "#创建词袋的持久化\n",
    "\n",
    "space_path=\"test_word_bag/testspace.dat\"\n",
    "\n",
    "writebunchobj(space_path,testspace)\n",
    "下面执行多项式贝叶斯算法进行测试文本分类并返回精度，代码如下：\n",
    "\n",
    " \n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB  # 导入多项式贝叶斯算法包\n",
    "\n",
    "\n",
    "\n",
    "def readbunchobj(path):\n",
    "\n",
    "    file_obj = open(path, \"rb\")\n",
    "\n",
    "    bunch = pickle.load(file_obj)\n",
    "\n",
    "    file_obj.close()\n",
    "\n",
    "    return bunch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 导入训练集向量空间\n",
    "\n",
    "trainpath = \"train_word_bag/tfidfspace.dat\"\n",
    "\n",
    "train_set = readbunchobj(trainpath)\n",
    "\n",
    "# d导入测试集向量空间\n",
    "\n",
    "testpath = \"test_word_bag/testspace.dat\"\n",
    "\n",
    "test_set = readbunchobj(testpath)\n",
    "\n",
    "# 应用贝叶斯算法\n",
    "\n",
    "# alpha:0.001 alpha 越小，迭代次数越多，精度越高\n",
    "\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "\n",
    "\n",
    "# 预测分类结果\n",
    "\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "\n",
    "total = len(predicted);rate = 0\n",
    "\n",
    "for flabel, file_name, expct_cate in zip(test_set.label, test_set.filenames, predicted):\n",
    "\n",
    "    if flabel != expct_cate:\n",
    "\n",
    "        rate += 1\n",
    "\n",
    "        print(file_name, \": 实际类别：\", flabel, \"-->预测分类：\", expct_cate)\n",
    "\n",
    "# 精度\n",
    "\n",
    "print(\"error_rate:\", float(rate) * 100 / float(total), \"%\")\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "6.分类结果评估\n",
    "\n",
    "机器学习领域的算法评估有三个基本指标：\n",
    "\n",
    "· 召回率（recall rate,查全率）：是检索出的相关文档数与文档库中所有相关文档的比率，衡量的是检索系统的查全率\n",
    "召回率=系统检索到的相关文件/系统所有相关的文件综述\n",
    "\n",
    "· 准确率（Precision，精度）：是检索出的相关文档数于检索出的文档总数的比率，衡量的是检索系统的查准率\n",
    "准确率=系统检索到的相关文件/系统所有的检索到的文件数\n",
    "\n",
    "准确率和召回率是相互影响的，理想情况下是二者都高，但是一般情况下准确率高，召回率就低；召回率高，准确率就低\n",
    "\n",
    "· F-Score（）：计算公式为：\n",
    " \n",
    "\n",
    "当=1时就是最常见的-Measure\n",
    "\n",
    "三者关系如下：\n",
    "\n",
    " \n",
    "\n",
    "具体评估代码如下：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#评估\n",
    "\n",
    "def metrics_result(actual,predict):\n",
    "\n",
    "    print(\"精度：{0:.3f}\".format(metrics.precision_score(actual,predict)))\n",
    "\n",
    "    print(\"召回：{0:0.3f}\".format(metrics.recall_score(actual,predict)))\n",
    "\n",
    "    print(\"f1-score:{0:.3f}\".format(metrics.f1_score(actual,predict)))\n",
    "\n",
    "metrics_result(test_set.label,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
